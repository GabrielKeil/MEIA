---
title: "PCA and Outlier Analysis on Machines Data"
author: Grupo
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
  pdf:
    toc: true
    toc-depth: 3
number-sections: true
execute:
  echo: true
  warning: false
  message: false
editor: visual
---

# Introduction

This report walks through an analysis of a subset of the `machines` data from the `rrcov` package (rows **hp-3000/64** to **ibm-4331-2**).

Goals:

\- Describe the variables with classical and robust summaries and simple visuals.

\- Run PCA on the raw scale and on standardized variables; keep ≥95% total variance.

\- Study the effect of an injected outlier using classical PCA and a robust PCA (MCD).

# Study plan (at a glance)

1)  Inspect the data: structure, variable meaning, rough scales.\
2)  Classical and robust summaries to see skew/tails and dependence (covariance, distances).\
3)  PCA on raw vs standardized variables; compare variance explained and loadings; pick components reaching 95%.\
4)  Inject one outlier and compare classical vs robust PCA to illustrate sensitivity.\
5)  Conclude and state a recommendation for which PCA to report and why.

# Setup and data

```{r}
# install.packages(c("rrcov", "robustbase"), repos = "https://cloud.r-project.org")
library(rrcov)
library(robustbase)

data(machines)
machines_sub <- machines[71:111, ]                  # hp-3000/64 ... ibm-4331-2
machines_sub$machine <- rownames(machines_sub)      # keep names for plotting
rownames(machines_sub) <- NULL

n_obs <- nrow(machines_sub)
n_vars <- ncol(machines_sub) - 1                    # numeric variables only

head(machines_sub, 3)
```

```{r}
summary(machines_sub)
```

# Data description

-   Observations: `r n_obs` machines; numeric variables: `r n_vars`.
-   Variables (all numeric):
    -   `MYCT` cycle time (ns), `MMIN` min memory (KB), `MMAX` max memory (KB), `CACH` cache (KB),\
    -   `CHMIN` min channels, `CHMAX` max channels, `PRP` published perf, `ERP` estimated perf.\
-   Machine IDs live in `machine` (formerly row names). Use them for labels, not for analysis.

Quick look at the machine names:

```{r}
head(machines_sub$machine, 5)
```

Helper functions for trimmed/winsorized means and total/generalized variance:

```{r}
winsor_mean <- function(x, probs = c(0.05, 0.95)) {
  qs <- quantile(x, probs, names = FALSE)
  mean(pmin(pmax(x, qs[1]), qs[2]))
}

total_variance <- function(S) sum(diag(S))
generalized_variance <- function(S) determinant(S, logarithm = TRUE)$modulus
```

# Exploratory summaries

## Classical vs robust location/scale

Focus: see how skew and heavy tails affect means; compare to median/trimmed/winsor/MAD.

```{r}
num_vars <- machines_sub[ , setdiff(names(machines_sub), "machine")]

stat_table <- data.frame(
  variable = names(num_vars),
  mean = sapply(num_vars, mean),
  median = sapply(num_vars, median),
  trimmed_mean = sapply(num_vars, mean, trim = 0.1),
  winsor_mean = sapply(num_vars, winsor_mean),
  sd = sapply(num_vars, sd),
  var = sapply(num_vars, var),
  mad = sapply(num_vars, mad)
)

stat_table
```

## Covariance, total and generalized variance

These give a sense of joint spread; generalized variance is the log-determinant (stable on the log scale).

```{r}
S_classic <- cov(num_vars)
total_var <- total_variance(S_classic)
gen_var_log <- generalized_variance(S_classic)  # log-determinant for stability

list(
  covariance_matrix = S_classic,
  total_variance = total_var,
  generalized_variance_log = gen_var_log
)
```

## Mahalanobis distances (classical)

Note: the squared Mahalanobis distance is approximately chi-square with `p` degrees of freedom (`p` = number of numeric variables). Using the 97.5th percentile cutoff marks points that are unusually far from the multivariate center (potential joint outliers).

```{r}
md <- mahalanobis(
  num_vars,
  center = colMeans(num_vars),        # classic mean vector
  cov = S_classic                     # classic covariance matrix
)
cutoff <- qchisq(0.975, df = ncol(num_vars))  # 97.5% chi-square threshold

plot(md, pch = 19, main = "Mahalanobis distances", ylab = "Distance")
abline(h = cutoff, col = "red", lty = 2)
```

Interpretation: most machines sit well below the 97.5% chi-square cutoff (≈17.5 for 8 vars), suggesting a fairly homogeneous core. About 5 machines breach the line (one extreme around index 25, another near 13, plus a few clustered neighbors), meaning their joint profiles are atypical – e.g., unusual combinations of cycle time, memory, and cache. Because classical mean/covariance can mask extremes, a robust Mahalanobis distance (MCD) would likely push these points even higher and could flag borderline cases too. To dig deeper, we can inspect those machine IDs and build contribution plots to see which variables drive their distances.

Which machines exceed the cutoff?

```{r}
machine_ids <- rownames(machines)[71:111]  # names for this slice of the data
flagged <- which(md > cutoff)

data.frame(
  index = flagged,
  machine = machine_ids[flagged],
  distance = round(md[flagged], 2)
)
```

These IDs correspond to the spikes in the plot; each mixes unusually large/small values across the hardware specs. Next step: contribution plots (variable-wise breakdown of the distance) or a robust Mahalanobis distance to confirm and prioritize which are true multivariate outliers.

# Principal Component Analysis (original vs standardized)

We compare PCA on raw scales (keeps original units) vs standardized (puts variables on equal footing).

```{r}
pca_raw <- prcomp(num_vars, center = TRUE, scale. = FALSE)
pca_std <- prcomp(num_vars, center = TRUE, scale. = TRUE)

pve_raw <- pca_raw$sdev^2 / sum(pca_raw$sdev^2)
pve_std <- pca_std$sdev^2 / sum(pca_std$sdev^2)
```

## Scree plots and variance explained

Red line marks 95% cumulative variance target.

```{r}
par(mfrow = c(1, 2))
plot(pve_raw * 100, type = "b", pch = 19, xlab = "PC", ylab = "% variance",
     main = "Raw scale")
lines(cumsum(pve_raw) * 100, type = "b", col = "blue")
abline(h = 95, col = "red", lty = 2)

plot(pve_std * 100, type = "b", pch = 19, xlab = "PC", ylab = "% variance",
     main = "Standardized")
lines(cumsum(pve_std) * 100, type = "b", col = "blue")
abline(h = 95, col = "red", lty = 2)
par(mfrow = c(1, 1))
```

```{r}
k_raw <- which(cumsum(pve_raw) >= 0.95)[1]
k_std <- which(cumsum(pve_std) >= 0.95)[1]

data.frame(
  scale = c("raw", "standardized"),
  pcs_needed_for_95pct = c(k_raw, k_std),
  cumulative_variance = c(cumsum(pve_raw)[k_raw], cumsum(pve_std)[k_std])
)
```

## Loadings and interpretation aids

Inspect which variables drive the first PCs (signs may flip without changing interpretation).

```{r}
head(pca_raw$rotation[, 1:min(3, ncol(num_vars))])
head(pca_std$rotation[, 1:min(3, ncol(num_vars))])
```

## Scores plot (first two PCs)

Use labels to spot grouping and extremes; standardized version is shown here.

```{r}
scores_std <- as.data.frame(pca_std$x)
scores_std$machine <- machines_sub$machine

plot(scores_std$PC1, scores_std$PC2, pch = 19,
     xlab = "PC1 (std)", ylab = "PC2 (std)",
     main = "Scores on standardized data")
text(scores_std$PC1, scores_std$PC2, labels = scores_std$machine,
     pos = 3, cex = 0.6)
```

# Outlier experiment

Introduce the atypical point at the former `hp-3000/64` row (no standardization).

```{r}
xnew <- c(75, 2000, 0.8, 80000, 300, 24, 62, 47)
machines_out <- machines_sub
machines_out[machines_out$machine == "hp-3000/64", names(num_vars)] <- xnew

pca_out_classic <- prcomp(machines_out[names(num_vars)], center = TRUE, scale. = FALSE)
pca_out_robust <- PcaCov(machines_out[names(num_vars)], cov.control = CovControlMcd(), scale = FALSE)

pve_out_classic <- pca_out_classic$sdev^2 / sum(pca_out_classic$sdev^2)
pve_out_robust <- pca_out_robust@eigenvalues / sum(pca_out_robust@eigenvalues)

data.frame(
  component = seq_along(pve_out_classic),
  classic_pct = round(pve_out_classic * 100, 2),
  robust_pct = round(pve_out_robust * 100, 2)
)
```

## Visual comparison: scores

Outlier effect: in classical PCA, the first axis often aligns with the extreme point; robust PCA should stay closer to the bulk structure.

```{r}
par(mfrow = c(1, 2))
plot(pca_out_classic$x[,1], pca_out_classic$x[,2], pch = 19,
     main = "Classic PCA with outlier", xlab = "PC1", ylab = "PC2")
text(pca_out_classic$x[,1], pca_out_classic$x[,2],
     labels = machines_out$machine, pos = 3, cex = 0.5)

plot(pca_out_robust@scores[,1], pca_out_robust@scores[,2], pch = 19,
     main = "Robust PCA (MCD)", xlab = "PC1", ylab = "PC2")
text(pca_out_robust@scores[,1], pca_out_robust@scores[,2],
     labels = machines_out$machine, pos = 3, cex = 0.5)
par(mfrow = c(1, 1))
```

### Distance diagnostics (robust PCA)

```{r}
plot(pca_out_robust)  # outlier map: orthogonal vs score distances
```

# Decisions and conclusions

-   Classical summaries provide a baseline; compare trimmed/winsorized means and MAD to flag skew/heavy tails.
-   PCA on raw scale vs standardized: choose the variant that reaches ≥95% variance with fewer components and clearer loadings; document which variables dominate each retained PC.
-   With the injected outlier, classical PCA reorients strongly toward the extreme point (variance inflation, first PC dominated by the outlier). The MCD-based PCA keeps stable directions and variance shares.
-   For reporting: state which PCA you recommend (often standardized if scales differ), how many components you keep (based on the 95% rule), and how the outlier analysis supports the robustness choice.

Suggestion for the written summary (fill in after rendering): - Data shape and key variable ranges; any clear skew or heavy tails. - Which PCA you selected (raw vs standardized), how many PCs kept for 95%, and how you interpret PC1/PC2 loadings. - What changed after adding the outlier; how robust PCA mitigated it. - Brief limitations (small sample, historical hardware data) and next steps (e.g., more robust checks or alternative outlier rules).

# Bibliography

-   Jolliffe, I. T. (2002). *Principal Component Analysis*. Springer.
-   Todorov, V., & Filzmoser, P. (2009). An object oriented framework for robust multivariate analysis. *Journal of Statistical Software*, 32(3), 1–47. \*\*\*
